{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HK074FYkNwTi"
   },
   "source": [
    "# Deep Learning with PyTorch Step-by-Step: A Beginner's Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edtdvwWONwTl"
   },
   "source": [
    "# Chapter 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8kZ1PM4NwTm",
    "outputId": "e7e2243e-b439-498a-ec02-b44b731ea643"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     11\u001b[0m config_chapter10()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# This is needed to render the plots in this chapter\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    import requests\n",
    "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open('config.py', 'wb').write(r.content)\n",
    "except ModuleNotFoundError:\n",
    "    pass\n",
    "\n",
    "from config import *\n",
    "config_chapter10()\n",
    "# This is needed to render the plots in this chapter\n",
    "from plots.chapter8 import *\n",
    "from plots.chapter9 import *\n",
    "from plots.chapter10 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JyxowqR5NwTp",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "from torchvision.transforms.v2 import Compose, Normalize, Pad\n",
    "\n",
    "from data_generation.square_sequences import generate_sequences\n",
    "from data_generation.image_classification import generate_dataset\n",
    "from helpers import index_splitter, make_balanced_sampler\n",
    "from stepbystep.v4 import StepByStep\n",
    "# These are the classes we built in Chapter 9\n",
    "from seq2seq import PositionalEncoding, subsequent_mask, EncoderDecoderSelfAttn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7lEypJINwTq"
   },
   "source": [
    "# Transform and Roll Out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZ5tuOx2NwTr"
   },
   "source": [
    "## Narrow Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBiNshjSNwTs"
   },
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdxrUQNTNwTt"
   },
   "source": [
    "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/attn_narrow_transf.png?raw=1)\n",
    "\n",
    "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/multihead_chunking.png?raw=1)\n",
    "\n",
    "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/attn_narrow_first_head.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8LNjGjUNwTu"
   },
   "source": [
    "### Multiheaded Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "mLiecSFaNwTu",
    "outputId": "1cff31dc-8853-4376-e148-aab4e5eddc37"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMultiHeadedAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_heads, d_model, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(MultiHeadedAttention, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = int(d_model / n_heads)\n",
    "        self.linear_query = nn.Linear(d_model, d_model)\n",
    "        self.linear_key = nn.Linear(d_model, d_model)\n",
    "        self.linear_value = nn.Linear(d_model, d_model)\n",
    "        self.linear_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.alphas = None\n",
    "\n",
    "    def make_chunks(self, x):\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        # N, L, D -> N, L, n_heads * d_k\n",
    "        x = x.view(batch_size, seq_len, self.n_heads, self.d_k)\n",
    "        # N, n_heads, L, d_k\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def init_keys(self, key):\n",
    "        # N, n_heads, L, d_k\n",
    "        self.proj_key = self.make_chunks(self.linear_key(key))\n",
    "        self.proj_value = self.make_chunks(self.linear_value(key))\n",
    "\n",
    "    def score_function(self, query):\n",
    "        # scaled dot product\n",
    "        # N, n_heads, L, d_k x # N, n_heads, d_k, L -> N, n_heads, L, L\n",
    "        proj_query = self.make_chunks(self.linear_query(query))\n",
    "        dot_products = torch.matmul(proj_query,\n",
    "                                    self.proj_key.transpose(-2, -1))\n",
    "        scores =  dot_products / np.sqrt(self.d_k)\n",
    "        return scores\n",
    "\n",
    "    def attn(self, query, mask=None):\n",
    "        # Query is batch-first: N, L, D\n",
    "        # Score function will generate scores for each head\n",
    "        scores = self.score_function(query) # N, n_heads, L, L\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        alphas = F.softmax(scores, dim=-1) # N, n_heads, L, L\n",
    "        alphas = self.dropout(alphas)\n",
    "        self.alphas = alphas.detach()\n",
    "\n",
    "        # N, n_heads, L, L x N, n_heads, L, d_k -> N, n_heads, L, d_k\n",
    "        context = torch.matmul(alphas, self.proj_value)\n",
    "        return context\n",
    "\n",
    "    def output_function(self, contexts):\n",
    "        # N, L, D\n",
    "        out = self.linear_out(contexts) # N, L, D\n",
    "        return out\n",
    "\n",
    "    def forward(self, query, mask=None):\n",
    "        if mask is not None:\n",
    "            # N, 1, L, L - every head uses the same mask\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # N, n_heads, L, d_k\n",
    "        context = self.attn(query, mask=mask)\n",
    "        # N, L, n_heads, d_k\n",
    "        context = context.transpose(1, 2).contiguous()\n",
    "        # N, L, n_heads * d_k = N, L, d_model\n",
    "        context = context.view(query.size(0), -1, self.d_model)\n",
    "        # N, L, d_model\n",
    "        out = self.output_function(context)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPndJg1cNwTx"
   },
   "outputs": [],
   "source": [
    "dummy_points = torch.randn(16, 2, 4) # N, L, F\n",
    "mha = MultiHeadedAttention(n_heads=2, d_model=4, dropout=0.0)\n",
    "mha.init_keys(dummy_points)\n",
    "out = mha(dummy_points) # N, L, D\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtuAVgk8NwTy"
   },
   "source": [
    "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/attn_narrow_2heads.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYhQbFhCNwTz"
   },
   "source": [
    "## Stacking Encoders and Decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3Xk9MI_NwTz"
   },
   "source": [
    "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/stacked_encdec.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAPDj4UeNwT0"
   },
   "source": [
    "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/stacked_layers.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2eXfMjCNwT0"
   },
   "source": [
    "## Wrapping \"Sub-Layers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVCV9AG4NwT0"
   },
   "source": [
    "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/sublayer.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luPQVuJKNwT1"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\begin{aligned}\n",
    "&\\text{outputs}_{\\text{norm-last}}=&\\text{norm(inputs + dropout(sublayer(inputs))}\n",
    "\\\\\n",
    "&\\text{outputs}_{\\text{norm-first}}=&\\text{inputs + dropout(sublayer(norm(inputs)))}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_Ke5z7WNwT1"
   },
   "source": [
    "## Transformer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXYoPU3UNwT2"
   },
   "source": [
    "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/enc_both.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVPRvS0tNwT2"
   },
   "source": [
    "$$\n",
    "\\large\n",
    "\\begin{aligned}\n",
    "&\\text{outputs}_{\\text{norm-last}}=&\\text{norm}(\\underbrace{\\text{norm(inputs + att(inputs))}}_{\\text{Output of SubLayer}_0} + \\text{ffn}(\\underbrace{\\text{norm(inputs + att(inputs))}}_{\\text{Output of SubLayer}_0}))\n",
    "\\\\\n",
    "\\\\\n",
    "&\\text{outputs}_{\\text{norm-first}}=&\\underbrace{\\text{inputs + att(norm(inputs))}}_{\\text{Output of SubLayer}_0}+\\text{ffn(norm(}\\underbrace{\\text{inputs + att(norm(inputs))}}_{\\text{Output of SubLayer}_0}))\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYdXRKjbNwT3"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, ff_units, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.ff_units = ff_units\n",
    "        self.self_attn_heads = MultiHeadedAttention(n_heads, d_model,\n",
    "                                                    dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_units, d_model),\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, mask=None):\n",
    "        # Sublayer #0\n",
    "        # Norm\n",
    "        norm_query = self.norm1(query)\n",
    "        # Multi-headed Attention\n",
    "        self.self_attn_heads.init_keys(norm_query)\n",
    "        states = self.self_attn_heads(norm_query, mask)\n",
    "        # Add\n",
    "        att = query + self.drop1(states)\n",
    "\n",
    "        # Sublayer #1\n",
    "        # Norm\n",
    "        norm_att = self.norm2(att)\n",
    "        # Feed Forward\n",
    "        out = self.ffn(norm_att)\n",
    "        # Add\n",
    "        out = att + self.drop2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ex52QCuNwT3"
   },
   "outputs": [],
   "source": [
    "class EncoderTransf(nn.Module):\n",
    "    def __init__(self, encoder_layer, n_layers=1, max_len=100):\n",
    "        super().__init__()\n",
    "        self.d_model = encoder_layer.d_model\n",
    "        self.pe = PositionalEncoding(max_len, self.d_model)\n",
    "        self.norm = nn.LayerNorm(self.d_model)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, query, mask=None):\n",
    "        # Positional Encoding\n",
    "        x = self.pe(query)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        # Norm\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYcNyRvgNwT4"
   },
   "outputs": [],
   "source": [
    "enclayer = nn.TransformerEncoderLayer(d_model=6, nhead=3, dim_feedforward=20)\n",
    "enctransf = nn.TransformerEncoder(enclayer, num_layers=1, norm=nn.LayerNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZHPrqBeNwT4"
   },
   "source": [
    "## Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMBfkO6KNwT5"
   },
   "source": [
    "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/dec_both.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IihqH32DNwT5"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, ff_units, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.ff_units = ff_units\n",
    "        self.self_attn_heads = MultiHeadedAttention(n_heads, d_model,\n",
    "                                                    dropout=dropout)\n",
    "        self.cross_attn_heads = MultiHeadedAttention(n_heads, d_model,\n",
    "                                                     dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_units, d_model),\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "\n",
    "    def init_keys(self, states):\n",
    "        self.cross_attn_heads.init_keys(states)\n",
    "\n",
    "    def forward(self, query, source_mask=None, target_mask=None):\n",
    "        # Sublayer #0\n",
    "        # Norm\n",
    "        norm_query = self.norm1(query)\n",
    "        # Masked Multi-head Attention\n",
    "        self.self_attn_heads.init_keys(norm_query)\n",
    "        states = self.self_attn_heads(norm_query, target_mask)\n",
    "        # Add\n",
    "        att1 = query + self.drop1(states)\n",
    "\n",
    "        # Sublayer #1\n",
    "        # Norm\n",
    "        norm_att1 = self.norm2(att1)\n",
    "        # Multi-head Attention\n",
    "        encoder_states = self.cross_attn_heads(norm_att1, source_mask)\n",
    "        # Add\n",
    "        att2 = att1 + self.drop2(encoder_states)\n",
    "\n",
    "        # Sublayer #2\n",
    "        # Norm\n",
    "        norm_att2 = self.norm3(att2)\n",
    "        # Feed Forward\n",
    "        out = self.ffn(norm_att2)\n",
    "        # Add\n",
    "        out = att2 + self.drop3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEtnxSOCNwT5"
   },
   "outputs": [],
   "source": [
    "class DecoderTransf(nn.Module):\n",
    "    def __init__(self, decoder_layer, n_layers=1, max_len=100):\n",
    "        super(DecoderTransf, self).__init__()\n",
    "        self.d_model = decoder_layer.d_model\n",
    "        self.pe = PositionalEncoding(max_len, self.d_model)\n",
    "        self.norm = nn.LayerNorm(self.d_model)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "    def init_keys(self, states):\n",
    "        for layer in self.layers:\n",
    "            layer.init_keys(states)\n",
    "\n",
    "    def forward(self, query, source_mask=None, target_mask=None):\n",
    "        # Positional Encoding\n",
    "        x = self.pe(query)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, source_mask, target_mask)\n",
    "        # Norm\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8syruFiKNwT6"
   },
   "outputs": [],
   "source": [
    "declayer = nn.TransformerDecoderLayer(d_model=6, nhead=3, dim_feedforward=20)\n",
    "dectransf = nn.TransformerDecoder(declayer, num_layers=1, norm=nn.LayerNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4AGPXYHNwT6"
   },
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNp8EWjyNwT7"
   },
   "outputs": [],
   "source": [
    "d_model = 4\n",
    "seq_len = 2\n",
    "n_points = 3\n",
    "\n",
    "torch.manual_seed(34)\n",
    "data = torch.randn(n_points, seq_len, d_model)\n",
    "pe = PositionalEncoding(seq_len, d_model)\n",
    "inputs = pe(data)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImijG1DsNwT7"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\overline{X}_{n,l} = \\frac{1}{D}\\sum_{d=1}^Dx_{n,l,d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vtGWClmNwT7"
   },
   "outputs": [],
   "source": [
    "inputs_mean = inputs.mean(axis=2).unsqueeze(2)\n",
    "inputs_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53ISCto4NwT7"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\sigma_{n,l}(X) = \\sqrt{\\frac{1}{D}\\sum_{d=1}^D(x_{n,l,d}-\\overline{X}_{n,l})^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwXyiLX6NwT8"
   },
   "outputs": [],
   "source": [
    "inputs_var = inputs.var(axis=2, unbiased=False).unsqueeze(2)\n",
    "inputs_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4bt0D6TNwT8"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{standardized}\\ x_{n,l,d}=\\frac{x_{n,l,d}-\\overline{X}_{n,l}}{\\sigma_{n,l}(X) + \\epsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZU-GKwHYNwT9"
   },
   "outputs": [],
   "source": [
    "(inputs - inputs_mean)/torch.sqrt(inputs_var+1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLBLX7RUNwT9"
   },
   "outputs": [],
   "source": [
    "layer_norm = nn.LayerNorm(d_model)\n",
    "layer_norm.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ej9uWpuhNwT9"
   },
   "outputs": [],
   "source": [
    "layer_norm = nn.LayerNorm(d_model)\n",
    "normalized = layer_norm(inputs)\n",
    "\n",
    "normalized[0][0].mean(), normalized[0][0].std(unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EcY83PUNwT-"
   },
   "outputs": [],
   "source": [
    "layer_norm.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ef34JrRgNwT_"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{layer normed}\\ x_{n,l,d} = b_d + w_d\\ \\text{standardized}\\ x_{n,l,d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HnjDVp_NwT_"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{layer normed}\\ x_{n,l,d} = \\text{standardized}\\ x_{n,l,d}\\ \\gamma_d + \\beta_d\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsXCvrZ0NwT_"
   },
   "source": [
    "### Batch vs Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URBonuuSNwT_"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(23)\n",
    "dummy_points = torch.randn(4, 1, 256)\n",
    "dummy_pe = PositionalEncoding(1, 256)\n",
    "dummy_enc = dummy_pe(dummy_points)\n",
    "dummy_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcxkCEjJNwUA"
   },
   "source": [
    "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/layer_vs_batch_norm.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKHBcHcTNwUA"
   },
   "outputs": [],
   "source": [
    "fig = hist_encoding(dummy_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPZ9lv8zNwUA"
   },
   "outputs": [],
   "source": [
    "layer_normalizer = nn.LayerNorm(256)\n",
    "dummy_normed = layer_normalizer(dummy_enc)\n",
    "dummy_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Ayl-iuSNwUB"
   },
   "outputs": [],
   "source": [
    "fig = hist_layer_normed(dummy_enc, dummy_normed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znS6-K7gNwUB"
   },
   "source": [
    "### Our Seq2Seq Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arObrQGSNwUB"
   },
   "outputs": [],
   "source": [
    "pe = PositionalEncoding(max_len=2, d_model=2)\n",
    "\n",
    "source_seq = torch.tensor([[[ 1.0349,  0.9661], [ 0.8055, -0.9169]]])\n",
    "source_seq_enc = pe(source_seq)\n",
    "source_seq_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoEKCgfGNwUC"
   },
   "outputs": [],
   "source": [
    "norm = nn.LayerNorm(2)\n",
    "norm(source_seq_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjctP2zQNwUC"
   },
   "source": [
    "### Projections or Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpaGfMKSNwUD"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(11)\n",
    "proj_dim = 6\n",
    "linear_proj = nn.Linear(2, proj_dim)\n",
    "pe = PositionalEncoding(2, proj_dim)\n",
    "\n",
    "source_seq_proj = linear_proj(source_seq)\n",
    "source_seq_proj_enc = pe(source_seq_proj)\n",
    "source_seq_proj_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILNhhrKaNwUD"
   },
   "outputs": [],
   "source": [
    "norm = nn.LayerNorm(proj_dim)\n",
    "norm(source_seq_proj_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfRNE7NrNwUD"
   },
   "source": [
    "## The Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXck3Bz7NwUD"
   },
   "source": [
    "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/full_transformer.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKwuPjG8NwUE"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderTransf(EncoderDecoderSelfAttn):\n",
    "    def __init__(self, encoder, decoder, input_len, target_len, n_features):\n",
    "        super(EncoderDecoderTransf, self).__init__(encoder, decoder, input_len, target_len)\n",
    "        self.n_features = n_features\n",
    "        self.proj = nn.Linear(n_features, encoder.d_model)\n",
    "        self.linear = nn.Linear(encoder.d_model, n_features)\n",
    "\n",
    "    def encode(self, source_seq, source_mask=None):\n",
    "        # Projection\n",
    "        source_proj = self.proj(source_seq)\n",
    "        encoder_states = self.encoder(source_proj, source_mask)\n",
    "        self.decoder.init_keys(encoder_states)\n",
    "\n",
    "    def decode(self, shifted_target_seq, source_mask=None, target_mask=None):\n",
    "        # Projection\n",
    "        target_proj = self.proj(shifted_target_seq)\n",
    "        outputs = self.decoder(target_proj,\n",
    "                               source_mask=source_mask,\n",
    "                               target_mask=target_mask)\n",
    "        # Linear\n",
    "        outputs = self.linear(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2MZn9BoNwUE"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderSelfAttn(nn.Module):\n",
    "    def __init__(self, encoder, decoder, input_len, target_len):\n",
    "        super(EncoderDecoderSelfAttn, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_len = input_len\n",
    "        self.target_len = target_len\n",
    "        self.trg_masks = self.subsequent_mask(self.target_len)\n",
    "\n",
    "    @staticmethod\n",
    "    def subsequent_mask(size):\n",
    "        attn_shape = (1, size, size)\n",
    "        subsequence_mask = (1 - torch.triu(torch.ones(attn_shape), diagonal=1))\n",
    "        return subsequent_mask\n",
    "\n",
    "    def encode(self, source_seq, source_mask):\n",
    "        encoder_states = self.encoder(source_seq, source_mask)\n",
    "        self.decoder.init_keys(encoder_states)\n",
    "\n",
    "    def decode(self, shifted_target_seq, source_mask=None, target_mask=None):\n",
    "        outputs = self.decoder(shifted_target_seq,\n",
    "                               source_mask=source_mask,\n",
    "                               target_mask=target_mask)\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, source_seq, source_mask):\n",
    "        inputs = source_seq[:, -1:]\n",
    "        for i in range(self.target_len):\n",
    "            out = self.decode(inputs, source_mask, self.trg_masks[:, :i+1, :i+1])\n",
    "            out = torch.cat([inputs, out[:, -1:, :]], dim=-2)\n",
    "            inputs = out.detach()\n",
    "        outputs = inputs[:, 1:, :]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, X, source_mask=None):\n",
    "        self.trg_masks = self.trg_masks.type_as(X).bool()\n",
    "        source_seq = X[:, :self.input_len, :]\n",
    "\n",
    "        self.encode(source_seq, source_mask)\n",
    "        if self.training:\n",
    "            shifted_target_seq = X[:, self.input_len-1:-1, :]\n",
    "            outputs = self.decode(shifted_target_seq, source_mask, self.trg_masks)\n",
    "        else:\n",
    "            outputs = self.predict(source_seq, source_mask)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoU52fKfNwUF"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9E19-2TjNwUF"
   },
   "outputs": [],
   "source": [
    "# Generating training data\n",
    "points, directions = generate_sequences(n=256, seed=13)\n",
    "full_train = torch.as_tensor(points).float()\n",
    "target_train = full_train[:, 2:]\n",
    "# Generating test data\n",
    "test_points, test_directions = generate_sequences(seed=17)\n",
    "full_test = torch.as_tensor(test_points).float()\n",
    "source_test = full_test[:, :2]\n",
    "target_test = full_test[:, 2:]\n",
    "# Datasets and data loaders\n",
    "train_data = TensorDataset(full_train, target_train)\n",
    "test_data = TensorDataset(source_test, target_test)\n",
    "generator = torch.Generator()\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True, generator=generator)\n",
    "test_loader = DataLoader(test_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYSUJjm9NwUF"
   },
   "outputs": [],
   "source": [
    "fig = plot_data(points, directions, n_rows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJguxfwmNwUG"
   },
   "source": [
    "### Model Configuration & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DF56cOibNwUG"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Layers\n",
    "enclayer = EncoderLayer(n_heads=3, d_model=6, ff_units=10, dropout=0.1)\n",
    "declayer = DecoderLayer(n_heads=3, d_model=6, ff_units=10, dropout=0.1)\n",
    "# Encoder and Decoder\n",
    "enctransf = EncoderTransf(enclayer, n_layers=2)\n",
    "dectransf = DecoderTransf(declayer, n_layers=2)\n",
    "# Transformer\n",
    "model_transf = EncoderDecoderTransf(enctransf, dectransf, input_len=2, target_len=2, n_features=2)\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_transf.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0IgwQWyNwUG"
   },
   "outputs": [],
   "source": [
    "for p in model_transf.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4LkgTLCNwUH"
   },
   "outputs": [],
   "source": [
    "sbs_seq_transf = StepByStep(model_transf, loss, optimizer)\n",
    "sbs_seq_transf.set_loaders(train_loader, test_loader)\n",
    "sbs_seq_transf.train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpCLDBgFNwUH"
   },
   "outputs": [],
   "source": [
    "fig = sbs_seq_transf.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNfDOfCUNwUH"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(11)\n",
    "x, y = next(iter(train_loader))\n",
    "device = sbs_seq_transf.device\n",
    "# Training\n",
    "model_transf.train()\n",
    "loss(model_transf(x.to(device)), y.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StSL5adGNwUH"
   },
   "outputs": [],
   "source": [
    "# Validation\n",
    "model_transf.eval()\n",
    "loss(model_transf(x.to(device)), y.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iANDCCINwUH"
   },
   "source": [
    "### Visualizing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQZb7OglNwUI"
   },
   "outputs": [],
   "source": [
    "fig = sequence_pred(sbs_seq_transf, full_test, test_directions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrS6ODOQNwUI"
   },
   "source": [
    "## The PyTorch Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEKEUT90NwUI"
   },
   "source": [
    "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/sublayer.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SudS3VazNwUI"
   },
   "outputs": [],
   "source": [
    "def encode_decode(self, source, target, source_mask=None, target_mask=None):\n",
    "    # Projections\n",
    "    src = self.preprocess(source)\n",
    "    tgt = self.preprocess(target)\n",
    "\n",
    "    out = self.transf(src, tgt,\n",
    "                      src_key_padding_mask=source_mask,\n",
    "                      tgt_mask=target_mask)\n",
    "\n",
    "    # Linear\n",
    "    out = self.linear(out) # N, L, F\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mT4DINoSNwUJ"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, transformer, input_len, target_len, n_features):\n",
    "        super().__init__()\n",
    "        self.transf = transformer\n",
    "        self.input_len = input_len\n",
    "        self.target_len = target_len\n",
    "        self.trg_masks = self.transf.generate_square_subsequent_mask(self.target_len)\n",
    "        self.n_features = n_features\n",
    "        self.proj = nn.Linear(n_features, self.transf.d_model)\n",
    "        self.linear = nn.Linear(self.transf.d_model, n_features)\n",
    "\n",
    "        max_len = max(self.input_len, self.target_len)\n",
    "        self.pe = PositionalEncoding(max_len, self.transf.d_model)\n",
    "        self.norm = nn.LayerNorm(self.transf.d_model)\n",
    "\n",
    "    def preprocess(self, seq):\n",
    "        seq_proj = self.proj(seq)\n",
    "        seq_enc = self.pe(seq_proj)\n",
    "        return self.norm(seq_enc)\n",
    "\n",
    "    def encode_decode(self, source, target, source_mask=None, target_mask=None):\n",
    "        # Projections\n",
    "        src = self.preprocess(source)\n",
    "        tgt = self.preprocess(target)\n",
    "\n",
    "        out = self.transf(src, tgt,\n",
    "                          src_key_padding_mask=source_mask,\n",
    "                          tgt_mask=target_mask)\n",
    "\n",
    "        # Linear\n",
    "        out = self.linear(out) # N, L, F\n",
    "        return out\n",
    "\n",
    "    def predict(self, source_seq, source_mask=None):\n",
    "        inputs = source_seq[:, -1:]\n",
    "        for i in range(self.target_len):\n",
    "            out = self.encode_decode(source_seq, inputs,\n",
    "                                     source_mask=source_mask,\n",
    "                                     target_mask=self.trg_masks[:i+1, :i+1])\n",
    "            out = torch.cat([inputs, out[:, -1:, :]], dim=-2)\n",
    "            inputs = out.detach()\n",
    "        outputs = out[:, 1:, :]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, X, source_mask=None):\n",
    "        self.trg_masks = self.trg_masks.type_as(X)\n",
    "        source_seq = X[:, :self.input_len, :]\n",
    "\n",
    "        if self.training:\n",
    "            shifted_target_seq = X[:, self.input_len-1:-1, :]\n",
    "            outputs = self.encode_decode(source_seq, shifted_target_seq,\n",
    "                                         source_mask=source_mask,\n",
    "                                         target_mask=self.trg_masks)\n",
    "        else:\n",
    "            outputs = self.predict(source_seq, source_mask)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4R3-nGxNwUJ"
   },
   "source": [
    "### Model Configuration & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kq1DO_7kNwUK"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "transformer = nn.Transformer(d_model=6, nhead=3,\n",
    "                             num_encoder_layers=1, num_decoder_layers=1,\n",
    "                             dim_feedforward=20, dropout=0.1, batch_first=True)\n",
    "model_transformer = TransformerModel(transformer, input_len=2, target_len=2, n_features=2)\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_transformer.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FT_3rWblNwUK"
   },
   "outputs": [],
   "source": [
    "for p in model_transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2h8ZHESHNwUK"
   },
   "outputs": [],
   "source": [
    "sbs_seq_transformer = StepByStep(model_transformer, loss, optimizer)\n",
    "sbs_seq_transformer.set_loaders(train_loader, test_loader)\n",
    "sbs_seq_transformer.train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqQkl4IrNwUK"
   },
   "outputs": [],
   "source": [
    "fig = sbs_seq_transformer.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSr-hYK1NwUL"
   },
   "source": [
    "### Visualizing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F34CjIPMNwUM"
   },
   "outputs": [],
   "source": [
    "fig = sequence_pred(sbs_seq_transformer, full_test, test_directions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1nna3PdNwUM"
   },
   "source": [
    "## Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYI3KD0JNwUM"
   },
   "source": [
    "### Data Generation & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBpa0M2kNwUM"
   },
   "outputs": [],
   "source": [
    "images, labels = generate_dataset(img_size=12, n_images=1000, binary=False, seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qc3DcJ3rNwUN"
   },
   "outputs": [],
   "source": [
    "img = torch.as_tensor(images[2]).unsqueeze(0).float()/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6dXu-7TNwUN"
   },
   "outputs": [],
   "source": [
    "fig = plot_images(img, title=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3sLRmR3NwUN"
   },
   "outputs": [],
   "source": [
    "class TransformedTensorDataset(Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "# Builds tensors from numpy arrays BEFORE split\n",
    "# Modifies the scale of pixel values from [0, 255] to [0, 1]\n",
    "x_tensor = torch.as_tensor(images / 255).float()\n",
    "y_tensor = torch.as_tensor(labels).long()\n",
    "\n",
    "# Uses index_splitter to generate indices for training and\n",
    "# validation sets\n",
    "train_idx, val_idx = index_splitter(len(x_tensor), [80, 20])\n",
    "# Uses indices to perform the split\n",
    "x_train_tensor = x_tensor[train_idx]\n",
    "y_train_tensor = y_tensor[train_idx]\n",
    "x_val_tensor = x_tensor[val_idx]\n",
    "y_val_tensor = y_tensor[val_idx]\n",
    "\n",
    "# We're not doing any data augmentation now\n",
    "train_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
    "val_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
    "\n",
    "# Uses custom dataset to apply composed transforms to each set\n",
    "train_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor, transform=train_composer)\n",
    "val_dataset = TransformedTensorDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n",
    "\n",
    "# Builds a weighted random sampler to handle imbalanced classes\n",
    "sampler = make_balanced_sampler(y_train_tensor)\n",
    "\n",
    "# Uses sampler in the training set to get a balanced data loader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=sampler)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHqOWqX4NwUO"
   },
   "source": [
    "### Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKVO_YStNwUO"
   },
   "source": [
    "#### Rearranging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDxnlTwcNwUO"
   },
   "outputs": [],
   "source": [
    "# Adapted from https://discuss.pytorch.org/t/tf-extract-image-patches-in-pytorch/43837\n",
    "def extract_image_patches(x, kernel_size, stride=1):\n",
    "    # Extract patches\n",
    "    patches = x.unfold(2, kernel_size, stride)\n",
    "    patches = patches.unfold(3, kernel_size, stride)\n",
    "    patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "\n",
    "    return patches.view(x.shape[0], patches.shape[1], patches.shape[2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UrluxHRNwUO"
   },
   "outputs": [],
   "source": [
    "kernel_size = 4\n",
    "patches = extract_image_patches(img, kernel_size, stride=kernel_size)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXtVbwyJNwUP"
   },
   "outputs": [],
   "source": [
    "fig = plot_patches(patches, kernel_size=kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skMZT66YNwUP"
   },
   "outputs": [],
   "source": [
    "seq_patches = patches.view(-1, patches.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wv4MDoIaNwUP"
   },
   "outputs": [],
   "source": [
    "fig = plot_seq_patches(seq_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRmNuEMVNwUP"
   },
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit_pytorch.py\n",
    "# !pip install einops\n",
    "# from einops import rearrange\n",
    "# patches = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
    "#                     p1 = kernel_size, p2 = kernel_size)\n",
    "# patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mtLFtYjNwUP"
   },
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kk1XD8GlNwUP"
   },
   "outputs": [],
   "source": [
    "# Adapted from https://amaarora.github.io/2021/01/18/ViT.html\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, dilation=1):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2AvFeBvNwUQ"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "patch_embed = PatchEmbed(img.size(-1), kernel_size, 1, kernel_size**2)\n",
    "embedded = patch_embed(img)\n",
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OY3ZcArZNwUR"
   },
   "outputs": [],
   "source": [
    "fig = plot_seq_patches(embedded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfFWwJiHNwUS"
   },
   "source": [
    "### Special Classifier Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36fTjAcpNwUS"
   },
   "outputs": [],
   "source": [
    "imgs = torch.as_tensor(images[2:4]).float()/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bnPbXlNNwUT"
   },
   "outputs": [],
   "source": [
    "fig = plot_images(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBPAYtSXNwUT"
   },
   "outputs": [],
   "source": [
    "embeddeds = patch_embed(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4U9NfRw8NwUT"
   },
   "outputs": [],
   "source": [
    "fig = plot_seq_patches_transp(embeddeds, add_cls=False, title='Image / Sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vd2-zqL4NwUT"
   },
   "outputs": [],
   "source": [
    "fig = plot_seq_patches_transp(embeddeds, add_cls=True, title='Image / Sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MekIClJxNwUU"
   },
   "outputs": [],
   "source": [
    "cls_token = nn.Parameter(torch.zeros(1, 1, 16))\n",
    "cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlwvhRoWNwUU"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "images.shape # N, C, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XryPjUspNwUU"
   },
   "outputs": [],
   "source": [
    "embed = patch_embed(images)\n",
    "embed.shape # N, L, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRQ2w07VNwUU"
   },
   "outputs": [],
   "source": [
    "cls_tokens = cls_token.expand(embed.size(0), -1, -1)\n",
    "embed_cls = torch.cat((cls_tokens, embed), dim=1)\n",
    "embed_cls.shape # N, L+1, D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ6MBraCNwUV"
   },
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dR2z9hjpNwUV"
   },
   "source": [
    "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/vit_model.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ri3i8FRzNwUV"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, encoder, img_size, in_channels, patch_size, n_outputs):\n",
    "        super().__init__()\n",
    "        self.d_model = encoder.d_model\n",
    "        self.n_outputs = n_outputs\n",
    "        self.encoder = encoder\n",
    "        self.mlp = nn.Linear(encoder.d_model, n_outputs)\n",
    "\n",
    "        self.embed = PatchEmbed(img_size, patch_size, in_channels, encoder.d_model)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, encoder.d_model))\n",
    "\n",
    "    def preprocess(self, X):\n",
    "        # Patch embeddings\n",
    "        # N, C, H, W -> N, L, D\n",
    "        src = self.embed(X)\n",
    "        # Special classifier token\n",
    "        # 1, 1, D -> N, 1, D\n",
    "        cls_tokens = self.cls_token.expand(X.size(0), -1, -1)\n",
    "        # Concatenates CLS tokens -> N, 1 + L, D\n",
    "        src = torch.cat((cls_tokens, src), dim=1)\n",
    "        return src\n",
    "\n",
    "    def encode(self, source):\n",
    "        # Encoder generates \"hidden states\"\n",
    "        states = self.encoder(source)\n",
    "        # Gets state from first token: CLS\n",
    "        cls_state = states[:, 0]  # N, 1, D\n",
    "        return cls_state\n",
    "\n",
    "    def forward(self, X):\n",
    "        src = self.preprocess(X)\n",
    "        # Featurizer\n",
    "        cls_state = self.encode(src)\n",
    "        # Classifier\n",
    "        out = self.mlp(cls_state) # N, 1, outputs\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyplvskgNwUV"
   },
   "source": [
    "### Model Configuration & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdCzmTlxNwUW"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(17)\n",
    "layer = EncoderLayer(n_heads=2, d_model=16, ff_units=20)\n",
    "encoder = EncoderTransf(layer, n_layers=1)\n",
    "model_vit = ViT(encoder, img_size=12, in_channels=1, patch_size=4, n_outputs=3)\n",
    "multi_loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer_vit = optim.Adam(model_vit.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6iLsGP9NwUW"
   },
   "outputs": [],
   "source": [
    "sbs_vit = StepByStep(model_vit, multi_loss_fn, optimizer_vit)\n",
    "sbs_vit.set_loaders(train_loader, val_loader)\n",
    "sbs_vit.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFVYbfTpNwUW"
   },
   "outputs": [],
   "source": [
    "fig = sbs_vit.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCdiBlVANwUW"
   },
   "outputs": [],
   "source": [
    "model_vit.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdHnee0bNwUW"
   },
   "outputs": [],
   "source": [
    "StepByStep.loader_apply(sbs_vit.val_loader, sbs_vit.correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-U-CDTv0NwUX"
   },
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h05IC47VNwUX"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoTuWHxQNwUX"
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "points, directions = generate_sequences(n=256, seed=13)\n",
    "full_train = torch.as_tensor(points).float()\n",
    "target_train = full_train[:, 2:]\n",
    "train_data = TensorDataset(full_train, target_train)\n",
    "generator = torch.Generator()\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True, generator=generator)\n",
    "\n",
    "# Validation/Test Set\n",
    "test_points, test_directions = generate_sequences(seed=17)\n",
    "full_test = torch.as_tensor(test_points).float()\n",
    "source_test = full_test[:, :2]\n",
    "target_test = full_test[:, 2:]\n",
    "test_data = TensorDataset(source_test, target_test)\n",
    "test_loader = DataLoader(test_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AO4kc1WxNwUe"
   },
   "source": [
    "### Model Assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ae4wnjfNwUe"
   },
   "source": [
    "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/full_transformer_and_class.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mx-VKEb1NwUe"
   },
   "source": [
    "#### 1. Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daCuCKi8NwUf"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderTransf(nn.Module):\n",
    "    def __init__(self, encoder, decoder, input_len, target_len, n_features):\n",
    "        super(EncoderDecoderTransf, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_len = input_len\n",
    "        self.target_len = target_len\n",
    "        self.trg_masks = self.subsequent_mask(self.target_len)\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.proj = nn.Linear(n_features, encoder.d_model)\n",
    "        self.linear = nn.Linear(encoder.d_model, n_features)\n",
    "\n",
    "    @staticmethod\n",
    "    def subsequent_mask(size):\n",
    "        attn_shape = (1, size, size)\n",
    "        subsequent_mask = (1 - torch.triu(torch.ones(attn_shape), diagonal=1))\n",
    "        return subsequent_mask\n",
    "\n",
    "    def encode(self, source_seq, source_mask=None):\n",
    "        # Projection\n",
    "        source_proj = self.proj(source_seq)\n",
    "        encoder_states = self.encoder(source_proj, source_mask)\n",
    "        self.decoder.init_keys(encoder_states)\n",
    "\n",
    "    def decode(self, shifted_target_seq, source_mask=None, target_mask=None):\n",
    "        # Projection\n",
    "        target_proj = self.proj(shifted_target_seq)\n",
    "        outputs = self.decoder(target_proj,\n",
    "                               source_mask=source_mask,\n",
    "                               target_mask=target_mask)\n",
    "        # Linear\n",
    "        outputs = self.linear(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, source_seq, source_mask):\n",
    "        inputs = source_seq[:, -1:]\n",
    "        for i in range(self.target_len):\n",
    "            out = self.decode(inputs, source_mask, self.trg_masks[:, :i+1, :i+1])\n",
    "            out = torch.cat([inputs, out[:, -1:, :]], dim=-2)\n",
    "            inputs = out.detach()\n",
    "        outputs = inputs[:, 1:, :]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, X, source_mask=None):\n",
    "        self.trg_masks = self.trg_masks.type_as(X).bool()\n",
    "        source_seq = X[:, :self.input_len, :]\n",
    "\n",
    "        self.encode(source_seq, source_mask)\n",
    "        if self.training:\n",
    "            shifted_target_seq = X[:, self.input_len-1:-1, :]\n",
    "            outputs = self.decode(shifted_target_seq, source_mask, self.trg_masks)\n",
    "        else:\n",
    "            outputs = self.predict(source_seq, source_mask)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pam5fHC7NwUg"
   },
   "source": [
    "#### 2. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_AaByiZNwUg"
   },
   "outputs": [],
   "source": [
    "class EncoderTransf(nn.Module):\n",
    "    def __init__(self, encoder_layer, n_layers=1, max_len=100):\n",
    "        super().__init__()\n",
    "        self.d_model = encoder_layer.d_model\n",
    "        self.pe = PositionalEncoding(max_len, self.d_model)\n",
    "        self.norm = nn.LayerNorm(self.d_model)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, query, mask=None):\n",
    "        # Positional Encoding\n",
    "        x = self.pe(query)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        # Norm\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5-Rvc97NwUg"
   },
   "source": [
    "#### 3. Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DllAVrJ7NwUh"
   },
   "outputs": [],
   "source": [
    "class DecoderTransf(nn.Module):\n",
    "    def __init__(self, decoder_layer, n_layers=1, max_len=100):\n",
    "        super(DecoderTransf, self).__init__()\n",
    "        self.d_model = decoder_layer.d_model\n",
    "        self.pe = PositionalEncoding(max_len, self.d_model)\n",
    "        self.norm = nn.LayerNorm(self.d_model)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "    def init_keys(self, states):\n",
    "        for layer in self.layers:\n",
    "            layer.init_keys(states)\n",
    "\n",
    "    def forward(self, query, source_mask=None, target_mask=None):\n",
    "        # Positional Encoding\n",
    "        x = self.pe(query)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, source_mask, target_mask)\n",
    "        # Norm\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-79xZjibNwUh"
   },
   "source": [
    "#### 4. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIqo4_VSNwUh"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        slope = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * slope) # even dimensions\n",
    "        pe[:, 1::2] = torch.cos(position * slope) # odd dimensions\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is N, L, D\n",
    "        # pe is 1, maxlen, D\n",
    "        scaled_x = x * np.sqrt(self.d_model)\n",
    "        encoded = scaled_x + self.pe[:, :x.size(1), :]\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feWr1KpKNwUh"
   },
   "source": [
    "#### 5. Encoder \"Layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVq71WuxNwUh"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, ff_units, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.ff_units = ff_units\n",
    "        self.self_attn_heads = MultiHeadedAttention(n_heads, d_model,\n",
    "                                                    dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_units, d_model),\n",
    "        )\n",
    "        self.sublayers = nn.ModuleList([SubLayerWrapper(d_model, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, query, mask=None):\n",
    "        # SubLayer 0 - Self-Attention\n",
    "        att = self.sublayers[0](query,\n",
    "                                sublayer=self.self_attn_heads,\n",
    "                                is_self_attn=True,\n",
    "                                mask=mask)\n",
    "        # SubLayer 1 - FFN\n",
    "        out = self.sublayers[1](att, sublayer=self.ffn)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoNvE43BNwUh"
   },
   "source": [
    "#### 6. Decoder \"Layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEFJ_P1ENwUi"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, ff_units, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.ff_units = ff_units\n",
    "        self.self_attn_heads = MultiHeadedAttention(n_heads, d_model,\n",
    "                                                    dropout=dropout)\n",
    "        self.cross_attn_heads = MultiHeadedAttention(n_heads, d_model,\n",
    "                                                     dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_units, d_model),\n",
    "        )\n",
    "        self.sublayers = nn.ModuleList([SubLayerWrapper(d_model, dropout) for _ in range(3)])\n",
    "\n",
    "    def init_keys(self, states):\n",
    "        self.cross_attn_heads.init_keys(states)\n",
    "\n",
    "    def forward(self, query, source_mask=None, target_mask=None):\n",
    "        # SubLayer 0 - Masked Self-Attention\n",
    "        att1 = self.sublayers[0](query,\n",
    "                                 sublayer=self.self_attn_heads,\n",
    "                                 is_self_attn=True,\n",
    "                                 mask=target_mask)\n",
    "        # SubLayer 1 - Cross-Attention\n",
    "        att2 = self.sublayers[1](att1,\n",
    "                                 sublayer=self.cross_attn_heads,\n",
    "                                 mask=source_mask)\n",
    "        # SubLayer 2 - FFN\n",
    "        out = self.sublayers[2](att2, sublayer=self.ffn)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvjwH1HDNwUi"
   },
   "source": [
    "#### 7. \"SubLayer\" Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKjDZ1FxNwUi"
   },
   "outputs": [],
   "source": [
    "class SubLayerWrapper(nn.Module):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer, is_self_attn=False, **kwargs):\n",
    "        norm_x = self.norm(x)\n",
    "        if is_self_attn:\n",
    "            sublayer.init_keys(norm_x)\n",
    "        out = x + self.drop(sublayer(norm_x, **kwargs))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHDgTly6NwUj"
   },
   "outputs": [],
   "source": [
    "# Before\n",
    "def forward(self, query, mask=None):\n",
    "    # query and mask go in\n",
    "    norm_query = self.norm1(query)\n",
    "    self.self_attn_heads.init_keys(norm_query)\n",
    "    # the sublayer is the self-attention\n",
    "    states = self.self_attn_heads(norm_query, mask)\n",
    "    att = query + self.drop1(states)\n",
    "    # att comes out\n",
    "    ...\n",
    "\n",
    "# After\n",
    "def forward(self, query, mask=None):\n",
    "    # query and mask go in\n",
    "    # the sublayer is the self-attention\n",
    "    # norm, drop, and residual are inside the wrapper\n",
    "    att = self.sublayers[0](query,\n",
    "                            sublayer=self.self_attn_heads,\n",
    "                            is_self_attn=True,\n",
    "                            mask=mask)\n",
    "    # att comes out\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZL83WGGNwUj"
   },
   "source": [
    "#### 8. Multi-Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fdt8QohNwUj"
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = int(d_model / n_heads)\n",
    "        self.linear_query = nn.Linear(d_model, d_model)\n",
    "        self.linear_key = nn.Linear(d_model, d_model)\n",
    "        self.linear_value = nn.Linear(d_model, d_model)\n",
    "        self.linear_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.alphas = None\n",
    "\n",
    "    def make_chunks(self, x):\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        # N, L, D -> N, L, n_heads * d_k\n",
    "        x = x.view(batch_size, seq_len, self.n_heads, self.d_k)\n",
    "        # N, n_heads, L, d_k\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def init_keys(self, key):\n",
    "        # N, n_heads, L, d_k\n",
    "        self.proj_key = self.make_chunks(self.linear_key(key))\n",
    "        self.proj_value = self.make_chunks(self.linear_value(key))\n",
    "\n",
    "    def score_function(self, query):\n",
    "        # scaled dot product\n",
    "        # N, n_heads, L, d_k x # N, n_heads, d_k, L -> N, n_heads, L, L\n",
    "        proj_query = self.make_chunks(self.linear_query(query))\n",
    "        dot_products = torch.matmul(proj_query,\n",
    "                                    self.proj_key.transpose(-2, -1))\n",
    "        scores =  dot_products / np.sqrt(self.d_k)\n",
    "        return scores\n",
    "\n",
    "    def attn(self, query, mask=None):\n",
    "        # Query is batch-first: N, L, D\n",
    "        # Score function will generate scores for each head\n",
    "        scores = self.score_function(query) # N, n_heads, L, L\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        alphas = F.softmax(scores, dim=-1) # N, n_heads, L, L\n",
    "        alphas = self.dropout(alphas)\n",
    "        self.alphas = alphas.detach()\n",
    "\n",
    "        # N, n_heads, L, L x N, n_heads, L, d_k -> N, n_heads, L, d_k\n",
    "        context = torch.matmul(alphas, self.proj_value)\n",
    "        return context\n",
    "\n",
    "    def output_function(self, contexts):\n",
    "        # N, L, D\n",
    "        out = self.linear_out(contexts) # N, L, D\n",
    "        return out\n",
    "\n",
    "    def forward(self, query, mask=None):\n",
    "        if mask is not None:\n",
    "            # N, 1, L, L - every head uses the same mask\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # N, n_heads, L, d_k\n",
    "        context = self.attn(query, mask=mask)\n",
    "        # N, L, n_heads, d_k\n",
    "        context = context.transpose(1, 2).contiguous()\n",
    "        # N, L, n_heads * d_k = N, L, d_model\n",
    "        context = context.view(query.size(0), -1, self.d_model)\n",
    "        # N, L, d_model\n",
    "        out = self.output_function(context)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0EbQOBmNwUj"
   },
   "source": [
    "### Model Configuration & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "relmLO6nNwUk"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Layers\n",
    "enclayer = EncoderLayer(n_heads=3, d_model=6, ff_units=10, dropout=0.1)\n",
    "declayer = DecoderLayer(n_heads=3, d_model=6, ff_units=10, dropout=0.1)\n",
    "# Encoder and Decoder\n",
    "enctransf = EncoderTransf(enclayer, n_layers=2)\n",
    "dectransf = DecoderTransf(declayer, n_layers=2)\n",
    "# Transformer\n",
    "model_transf = EncoderDecoderTransf(enctransf, dectransf, input_len=2, target_len=2, n_features=2)\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_transf.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNWKnLgANwUk"
   },
   "outputs": [],
   "source": [
    "for p in model_transf.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lcICEImNwUk"
   },
   "outputs": [],
   "source": [
    "sbs_seq_transf = StepByStep(model_transf, loss, optimizer)\n",
    "sbs_seq_transf.set_loaders(train_loader, test_loader)\n",
    "sbs_seq_transf.train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcXuFB_INwUk"
   },
   "outputs": [],
   "source": [
    "sbs_seq_transf.losses[-1], sbs_seq_transf.val_losses[-1]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Chapter10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
